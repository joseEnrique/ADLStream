{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io.arff as arff\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import arff as arff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '../../datasets_drift/'\n",
    "arff_files = glob.glob(os.path.join(dir_path, '**/*.arff'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_arff(file_path, data, attributes, relation):\n",
    "    f = open(file_path, \"w\")\n",
    "    \n",
    "    f.write('@relation \\'{0}\\' \\n\\n'.format(relation))\n",
    "    \n",
    "    for attr_name, attr_type in attributes:\n",
    "        if type(attr_type) == type([]):\n",
    "            attr_type = '{{{0}}}'.format(','.join(attr_type))\n",
    "        f.write('@attribute {0} {1} \\n'.format(attr_name, attr_type))\n",
    "    \n",
    "    f.write('\\n')\n",
    "    f.write('@data\\n')\n",
    "    f.write('\\n')\n",
    "    \n",
    "    for line in data:\n",
    "        f.write(','.join([str(x) for x in line])+',\\n')    \n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [06:02<00:00, 25.87s/it]\n"
     ]
    }
   ],
   "source": [
    "for file_path in tqdm(arff_files):\n",
    "    topic = os.path.basename(file_path).replace('.arff', '')\n",
    "    data, meta = arff.loadarff(file_path)\n",
    "    dataset = pd.DataFrame(data)\n",
    "    \n",
    "    class_column = dataset.pop('class')\n",
    "    class_column = class_column.str.decode('utf-8') if type(class_column[0]) == type(b'') else class_column\n",
    "\n",
    "    numeric_columns = [column for column in dataset.columns if meta[column][0] == 'numeric']\n",
    "    nominal_columns = [column for column in dataset.columns if meta[column][0] == 'nominal']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    dataset[numeric_columns] = scaler.fit_transform(dataset[numeric_columns])\n",
    "\n",
    "    for column in nominal_columns:\n",
    "        if type(dataset[column][0]) == type(b''):\n",
    "            dataset[column] = dataset[column].str.decode('utf-8')\n",
    "        onehotencoder = OneHotEncoder()\n",
    "        x = onehotencoder.fit_transform(dataset[[column]]).toarray()\n",
    "        new_columns = [column+'_'+str(i) for i in range(x.shape[1])]\n",
    "        for i, new_col in enumerate(new_columns):\n",
    "            dataset[new_col] = x[:,i]\n",
    "        dataset.pop(column)\n",
    "\n",
    "    dataset['class'] = class_column\n",
    "    \n",
    "    attributes = [(c, 'NUMERIC') for c in dataset.columns.values[:-1]]\n",
    "    attributes += [('class', dataset['class'].unique().astype(str).tolist())]\n",
    "    \n",
    "    data = dataset.values.tolist()\n",
    "        \n",
    "    write_arff(file_path, data, relation=topic, attributes=attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
