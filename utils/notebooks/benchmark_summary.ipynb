{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RESULT_DIR = ['../../DSClassificationResults/DSClassificationResults_keras', '../../DSClassificationResults/DSClassificationResults_sklearn', '../../DSClassificationResults/DSClassificationResults_MOA'] \n",
    "_MODEL = 'keras_parallel_3_Dilated_Conv'\n",
    "_METRICS_FILE_NAME = 'metrics.csv'\n",
    "_DATA_FILE_NAME = 'data.csv'\n",
    "_RES_FILE_PATH = './files/benchmark_sensitivityAnalysis.csv'\n",
    "#_DATASETS = sorted(os.listdir(_RESULT_DIR))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prequential_accuracy(data, fading_factor=0.98, chunk_size=10):\n",
    "    # get accuracy for each instance\n",
    "    accuracy = (data[data.columns[data.shape[1]-2]]==data[data.columns[data.shape[1]-1]]).astype(\"int\")\n",
    "    # every chunks should have the same size\n",
    "    accuracy = accuracy.iloc[:accuracy.shape[0]-(accuracy.shape[0]%chunk_size)]\n",
    "    # compute accuracy for each chunk\n",
    "    accuracy = [accuracy.iloc[start:end].mean() \n",
    "                for (start,end) in [\n",
    "                    (i*chunk_size, (i+1)*chunk_size) for i in range(int(accuracy.shape[0]/chunk_size))\n",
    "                ]\n",
    "               ]\n",
    "    # compute fading factor for each chunk\n",
    "    i = len(accuracy)\n",
    "    fading_factor_chunks = [fading_factor**(i-k) for k in range(1, i+1)]\n",
    "    fading_factor_sum = sum(fading_factor_chunks)\n",
    "    # Compute the final accuracy: sum the faded accuracies \n",
    "    accuracy_faded = sum([(fading_factor_chunks[k]*accuracy[k])/fading_factor_sum for k in range(len(accuracy))])\n",
    "    return accuracy_faded\n",
    "\n",
    "\n",
    "def prequential_kappa(data, fading_factor=0.98, chunk_size=10):\n",
    "    # every chunks should have the same size\n",
    "    data_clean =  data [[data.columns[data.shape[1]-2], data.columns[data.shape[1]-1]]].astype(int)\n",
    "    data_clean.columns = [\"Class\", \"Prediction\"]\n",
    "    # get kappa for each chunks\n",
    "    kappa = [cohen_kappa_score(data_clean.iloc[start:end][\"Class\"], data_clean.iloc[start:end][\"Prediction\"]) \n",
    "              for (start,end) in [\n",
    "                  (i*chunk_size, (i+1)*chunk_size) for i in range(int(data_clean.shape[0]/chunk_size))\n",
    "              ]\n",
    "             ]\n",
    "    # Change NaN for 1 (kappa is nan when there is only one class in the chunk)\n",
    "    kappa = [1. if math.isnan(k) else k for k in kappa]\n",
    "    # compute fading factor for each chunk\n",
    "    i = len(kappa)\n",
    "    fading_factor_chunks = [fading_factor**(i-k) for k in range(1, i+1)]\n",
    "    fading_factor_sum = sum(fading_factor_chunks)\n",
    "    # compute final kappa: sum the faded kappa\n",
    "    kappa_faded = sum([(fading_factor_chunks[k]*kappa[k])/fading_factor_sum for k in range(len(kappa))])\n",
    "    return kappa_faded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../DSClassificationResults/DSClassificationResults_keras:   8%|▊         | 7/92 [00:03<00:50,  1.70it/s]/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:370: RuntimeWarning: invalid value encountered in true_divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "../../DSClassificationResults/DSClassificationResults_keras: 100%|██████████| 92/92 [08:40<00:00, 22.91s/it]\n",
      "../../DSClassificationResults/DSClassificationResults_sklearn: 100%|██████████| 92/92 [14:00<00:00, 54.60s/it]\n",
      "../../DSClassificationResults/DSClassificationResults_MOA: 100%|██████████| 92/92 [1:01:20<00:00, 239.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors:  [('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_20x40', 'BeetleFly', \"File b'../../DSClassificationResults/DSClassificationResults_keras/BeetleFly/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_20x40/data.csv' does not exist\"), ('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_40x40', 'BeetleFly', \"File b'../../DSClassificationResults/DSClassificationResults_keras/BeetleFly/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_40x40/data.csv' does not exist\"), ('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_60x40', 'BeetleFly', \"File b'../../DSClassificationResults/DSClassificationResults_keras/BeetleFly/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_60x40/data.csv' does not exist\"), ('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_20x40', 'BirdChicken', \"File b'../../DSClassificationResults/DSClassificationResults_keras/BirdChicken/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_20x40/data.csv' does not exist\"), ('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_40x40', 'BirdChicken', \"File b'../../DSClassificationResults/DSClassificationResults_keras/BirdChicken/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_40x40/data.csv' does not exist\"), ('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_60x40', 'BirdChicken', \"File b'../../DSClassificationResults/DSClassificationResults_keras/BirdChicken/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_60x40/data.csv' does not exist\")]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "metrics_ls = []\n",
    "\n",
    "for result_dir in _RESULT_DIR:\n",
    "    datasets = sorted(os.listdir(result_dir))\n",
    "    for dataset in tqdm(datasets, desc=result_dir):\n",
    "        models = sorted(os.listdir(os.path.join(result_dir, dataset)))\n",
    "        for model in models:\n",
    "            # read files \n",
    "            metrics_file_path = os.path.join(result_dir, dataset, model, _METRICS_FILE_NAME)\n",
    "            data_file_path = os.path.join(result_dir, dataset, model, _DATA_FILE_NAME)\n",
    "            try:\n",
    "                if 'MOA' in model:\n",
    "                    data = pd.read_csv(data_file_path,header=None)\n",
    "                else:\n",
    "                    data = pd.read_csv(data_file_path,header=0)\n",
    "                metrics = pd.read_csv(metrics_file_path)\n",
    "                metrics.columns = [c.lower() for c in metrics.columns]\n",
    "            except Exception as e:\n",
    "                errors.append((model,dataset, str(e)))\n",
    "                continue\n",
    "            \n",
    "            # compute metrics\n",
    "            num_instances = data.shape[0]\n",
    "            num_attributes = data.shape[1]-2\n",
    "            num_classes = len(pd.unique(data[data.columns[data.shape[1]-2]].astype(int)))\n",
    "            \n",
    "            accuracy = prequential_accuracy(data)\n",
    "            kappa = prequential_kappa(data)\n",
    "            \n",
    "            train_time_mean = metrics['train_time_s'].mean() if 'train_time_s' in metrics else metrics['train_time'].mean()\n",
    "            test_time_mean = metrics['test_time_s'].mean() if 'test_time_s' in metrics else metrics['test_time'].mean()\n",
    "            total_time_mean = (metrics['train_time_s'] + metrics['test_time_s']).mean() if 'test_time_s' in metrics else (metrics['train_time'] + metrics['test_time']).mean()\n",
    "            \n",
    "            train_time = metrics['train_time_s'].sum() if 'train_time_s' in metrics else metrics['train_time'].sum()\n",
    "            test_time = metrics['test_time_s'].sum() if 'test_time_s' in metrics else metrics['test_time'].sum()\n",
    "            total_time = (metrics['train_time_s'].sum() + metrics['test_time_s'].sum() if 'parallel' not in model else max(metrics['train_time_s'].sum(), metrics['test_time_s'].sum())) if 'test_time_s' in metrics else (metrics['train_time'].sum() + metrics['test_time'].sum() if 'parallel' not in model else max(metrics['train_time'].sum(), metrics['test_time'].sum()))\n",
    "                \n",
    "            metrics_summary = {'dataset':dataset,\n",
    "                      'classifier':model,\n",
    "                      'instances': num_instances,\n",
    "                      'attributes': num_attributes,\n",
    "                      'classes': num_classes,\n",
    "                      'accuracy': accuracy,\n",
    "                      'kappa': kappa,\n",
    "                      'train_time_mean': train_time_mean,\n",
    "                      'test_time_mean': test_time_mean,\n",
    "                      'total_time_mean': total_time_mean,\n",
    "                      'train_time': train_time,\n",
    "                      'test_time': test_time,\n",
    "                      'total_time': total_time        \n",
    "                     }\n",
    "            metrics_ls.append(metrics_summary)\n",
    "        \n",
    "\n",
    "res = pd.DataFrame(metrics_ls, columns=['dataset','classifier','instances', 'attributes', 'classes','accuracy', 'kappa', 'train_time_mean', 'test_time_mean', 'total_time_mean', 'train_time', 'test_time', 'total_time'])\n",
    "res = res.sort_values(by=['dataset', 'classifier'])\n",
    "res.to_csv(_RES_FILE_PATH, index=False)\n",
    "\n",
    "if errors:\n",
    "    print(\"Errors: \" , errors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_20x40',\n",
       "  'BeetleFly',\n",
       "  \"File b'../../DSClassificationResults/DSClassificationResults_keras/BeetleFly/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_20x40/data.csv' does not exist\"),\n",
       " ('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_40x40',\n",
       "  'BeetleFly',\n",
       "  \"File b'../../DSClassificationResults/DSClassificationResults_keras/BeetleFly/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_40x40/data.csv' does not exist\"),\n",
       " ('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_60x40',\n",
       "  'BeetleFly',\n",
       "  \"File b'../../DSClassificationResults/DSClassificationResults_keras/BeetleFly/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_60x40/data.csv' does not exist\"),\n",
       " ('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_20x40',\n",
       "  'BirdChicken',\n",
       "  \"File b'../../DSClassificationResults/DSClassificationResults_keras/BirdChicken/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_20x40/data.csv' does not exist\"),\n",
       " ('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_40x40',\n",
       "  'BirdChicken',\n",
       "  \"File b'../../DSClassificationResults/DSClassificationResults_keras/BirdChicken/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_40x40/data.csv' does not exist\"),\n",
       " ('keras_parallel_keras_parallel_3_Dilated_Conv_pooling_60x40',\n",
       "  'BirdChicken',\n",
       "  \"File b'../../DSClassificationResults/DSClassificationResults_keras/BirdChicken/keras_parallel_keras_parallel_3_Dilated_Conv_pooling_60x40/data.csv' does not exist\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPRECATED - DO NOT USE\n",
    "#_BATCH_SIZE = 10\n",
    "#_NUM_BATCH_FED = 40\n",
    "#\n",
    "#errors = []\n",
    "#metrics_ls = []\n",
    "#\n",
    "#for result_dir in _RESULT_DIR:\n",
    "#    datasets = sorted(os.listdir(result_dir))\n",
    "#    for dataset in datasets:\n",
    "#        models = sorted(os.listdir(os.path.join(result_dir, dataset)))\n",
    "#        for model in models:               \n",
    "#            file_path = os.path.join(result_dir, dataset, model, _FILE_NAME)\n",
    "#            \n",
    "#            try:\n",
    "#                metrics = pd.read_csv(file_path)\n",
    "#            except Exception:\n",
    "#                errors.append(model + \" - \" + dataset)\n",
    "#                continue\n",
    "#            \n",
    "#            if metrics['total'].max() <= _BATCH_SIZE*_NUM_BATCH_FED:\n",
    "#                continue\n",
    "#            \n",
    "#            metrics = metrics[metrics['total'] > _BATCH_SIZE*_NUM_BATCH_FED]\n",
    "#            \n",
    "#            if model.startswith(\"MOA\"):\n",
    "#                metrics['accuracy'] = metrics['accuracy']/100. \n",
    "#                \n",
    "#            metrics_summary = {'dataset':dataset,\n",
    "#                      'classifier':model,\n",
    "#                      'total':metrics['total'].max(),\n",
    "#                      'tp':metrics['tp'].mean() if 'tp' in metrics.columns else \"\",\n",
    "#                      'tn':metrics['tn'].mean() if 'tn' in metrics.columns else \"\", \n",
    "#                      'fp':metrics['fp'].mean() if 'fp' in metrics.columns else \"\",\n",
    "#                      'fn':metrics['fn'].mean() if 'fn' in metrics.columns else \"\",\n",
    "#                      'precision':metrics['precision'].mean() if 'precision' in metrics.columns else \"\",\n",
    "#                      'recall':metrics['recall'].mean() if 'recall' in metrics.columns else \"\",\n",
    "#                      'f1':metrics['f1'].mean() if 'f1' in metrics.columns else \"\",\n",
    "#                      'fbeta':metrics['fbeta'].mean() if 'fbeta' in metrics.columns else \"\",\n",
    "#                      'accuracy':metrics['accuracy'].mean() if 'accuracy' in metrics.columns else \"\",\n",
    "#                      'train_time_s':metrics['train_time'].sum() if 'train_time' in metrics.columns else \"\",\n",
    "#                      'test_time_s':metrics['test_time'].sum() if 'test_time' in metrics.columns else \"\"\n",
    "#                     }\n",
    "#            metrics_ls.append(metrics_summary)\n",
    "#\n",
    "#res = pd.DataFrame(metrics_ls, columns=['dataset','classifier','total','tp','tn','fp','fn','precision','recall','f1','fbeta','accuracy','train_time_s','test_time_s'])\n",
    "#res = res.sort_values(by=['dataset', 'classifier'])\n",
    "#res.to_csv(_RES_FILE_PATH, index=False)\n",
    "#\n",
    "#if errors:\n",
    "#    print(\"Errors: \" + str(errors))\n",
    "#    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
